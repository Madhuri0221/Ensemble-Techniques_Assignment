{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer- #  Ensemble Learning in Machine Learning\n",
        "\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Step 1: Text explanation\n",
        "text = \"\"\"\n",
        "## Ensemble Learning in Machine Learning\n",
        "\n",
        "**Definition:**  \n",
        "Ensemble Learning is a machine learning technique where multiple models\n",
        "(often called \"weak learners\") are trained and combined to solve the same\n",
        "problem. The idea is that by aggregating predictions from multiple models,\n",
        "we can achieve better performance than any single model could on its own.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "Different models may make different errors. By combining them, the errors\n",
        "can cancel out, leading to a more accurate and robust final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "**Main Types of Ensemble Methods:**\n",
        "1. **Bagging (Bootstrap Aggregating):**  \n",
        "   - Trains multiple models in parallel on different random subsets of data.  \n",
        "   - Example: Random Forest.\n",
        "   \n",
        "2. **Boosting:**  \n",
        "   - Trains models sequentially, where each new model focuses on fixing\n",
        "     errors made by the previous ones.  \n",
        "   - Example: AdaBoost, XGBoost.\n",
        "   \n",
        "3. **Stacking:**  \n",
        "   - Combines predictions of multiple models using another \"meta-model\"\n",
        "     for the final prediction.\n",
        "\n",
        "---\n",
        "\n",
        "**Real-world Analogy:**  \n",
        "Think of asking multiple friends for their opinion before making a decision.  \n",
        "Even if each friend is not perfect, combining their opinions may lead to\n",
        "a better choice.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Tve34yA9HMoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer-\n",
        "\n",
        "```\n",
        "| Feature               | **Bagging**                                                                     | **Boosting**                                                                                                     |\n",
        "| --------------------- | ------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
        "| **Full Name**         | Bootstrap Aggregating                                                           | —                                                                                                                |\n",
        "| **Training Approach** | Trains multiple models **in parallel** on different random subsets of the data. | Trains models **sequentially**, each new model focuses on correcting errors of the previous one.                 |\n",
        "| **Data Sampling**     | Random sampling **with replacement** (bootstrapping).                           | Uses all data, but weights are adjusted so misclassified samples get higher weight in the next round.            |\n",
        "| **Model Weighting**   | All models have **equal weight** in final prediction.                           | Models are **weighted** based on their accuracy (better models have higher influence).                           |\n",
        "| **Goal**              | Reduce **variance** (avoids overfitting).                                       | Reduce **bias** and variance.                                                                                    |\n",
        "| **Common Algorithms** | Random Forest                                                                   | AdaBoost, Gradient Boosting, XGBoost                                                                             |\n",
        "| **Example Analogy**   | Ask 10 friends the same question independently and take the majority vote.      | Ask 1 friend, then a second friend to correct their mistakes, then a third to fix remaining mistakes, and so on. |\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "A8OKNaQYHr9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer- Here’s a clear breakdown you can directly use in your Google Colab notes:\n",
        "\n",
        "---\n",
        "\n",
        "## **Bootstrap Sampling**\n",
        "\n",
        "**Definition:**\n",
        "Bootstrap sampling is a statistical method where we create new datasets by randomly selecting samples **with replacement** from the original dataset.\n",
        "\n",
        "* “With replacement” means the same data point can appear multiple times in the new dataset.\n",
        "* Each bootstrap sample is usually the **same size** as the original dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Role in Bagging (e.g., Random Forest)**\n",
        "\n",
        "1. **Diversity in Models**\n",
        "\n",
        "   * In Bagging, each base learner (e.g., a decision tree in Random Forest) is trained on a **different bootstrap sample**.\n",
        "   * This diversity reduces correlation between models.\n",
        "\n",
        "2. **Variance Reduction**\n",
        "\n",
        "   * Since each model sees a slightly different dataset, their errors are less likely to be the same.\n",
        "   * Combining their predictions (e.g., majority vote) averages out the errors, reducing variance.\n",
        "\n",
        "3. **Helps Avoid Overfitting**\n",
        "\n",
        "   * Random subsets prevent every model from memorizing the same training data.\n",
        "\n",
        "---\n",
        "\n",
        "**Example:**\n",
        "Imagine a dataset of 100 rows. In bootstrap sampling:\n",
        "\n",
        "* We randomly pick 100 rows **with replacement**.\n",
        "* Some rows will appear more than once, some won’t appear at all.\n",
        "* This becomes the training set for one tree in the forest.\n",
        "\n",
        "---\n",
        "\n",
        "**Visual Analogy:**\n",
        "Think of a teacher creating several practice tests by **randomly reusing questions** from a master question bank. Each student (model) gets a slightly different test, so when they share answers (ensemble prediction), the final answer is more reliable."
      ],
      "metadata": {
        "id": "n56nje0iJG-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer- Here’s a **clear explanation** you can directly put into your Google Colab notes with examples.\n",
        "\n",
        "---\n",
        "\n",
        "## **Out-of-Bag (OOB) Samples & OOB Score**\n",
        "\n",
        "### **1. What are OOB Samples?**\n",
        "\n",
        "* In **bootstrap sampling** (used in Bagging/Random Forest), each model is trained on a bootstrap sample created **with replacement** from the original dataset.\n",
        "* On average, **about 63%** of the original data points are included in a given bootstrap sample.\n",
        "* The **remaining \\~37%** of the data points are **not** included in that bootstrap sample — these are called **Out-of-Bag (OOB) samples**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How OOB Score Works**\n",
        "\n",
        "* OOB samples act as a **built-in validation set** for each model.\n",
        "* For each observation:\n",
        "\n",
        "  1. Identify the models (trees) that **did not** train on it.\n",
        "  2. Use those models to predict its label.\n",
        "  3. Compare the prediction to the actual label.\n",
        "* The **OOB score** is the accuracy (or another metric) computed from these predictions, averaged over all observations.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why OOB Score is Useful**\n",
        "\n",
        "* Eliminates the need for a separate validation set.\n",
        "* Provides an **unbiased estimate** of model performance.\n",
        "* Saves data — especially useful when the dataset is small.\n",
        "\n",
        "---\n",
        "\n",
        "**Example in Random Forest:**\n",
        "\n",
        "* Train 100 decision trees using bootstrap samples.\n",
        "* For each sample, about 37% of data points are left out.\n",
        "* Predict these left-out points using only the trees that never saw them.\n",
        "* The overall accuracy is the **OOB score**.\n",
        "\n",
        "---\n",
        "\n",
        "**Quick Analogy:**\n",
        "Imagine 10 chefs making dishes from a cookbook, each skipping a few recipes. Later, those skipped recipes are judged only by the chefs who **never cooked them**. Their collective score is the **OOB score**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1lDY9BDhJnOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "Ans-\n",
        "\n",
        "```\n",
        "| Aspect                         | **Single Decision Tree**                                                                                                                                                     | **Random Forest**                                                                              |\n",
        "| ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
        "| **How Importance is Computed** | Based on **Gini Importance** (a.k.a. Mean Decrease in Impurity): Measures how much each feature reduces impurity (like Gini Index or Entropy) when it is used for splitting. | Same as a single tree (**Gini Importance**), but averaged over **all trees** in the forest.    |\n",
        "| **Data Used**                  | Uses **only one model**, so importance comes from the splits in that one tree.                                                                                               | Uses **multiple trees** trained on different bootstrap samples and random subsets of features. |\n",
        "| **Stability**                  | Less stable — small changes in data can drastically change which features are chosen and their importance ranking.                                                           | More stable — averaging across many trees reduces the effect of randomness or noise.           |\n",
        "| **Bias in Importance**         | Can be **biased towards features with more categories** (categorical) or higher variance (numerical).                                                                        | Bias still exists, but reduced because many trees and feature subsets are considered.          |\n",
        "| **Interpretability**           | Very easy to interpret (one model).                                                                                                                                          | Slightly harder to interpret, but gives **more reliable** importance rankings.                 |\n",
        "| **Overfitting Risk**           | High — especially if the tree is deep. Importance might reflect noise.                                                                                                       | Lower — aggregation smooths out overfitting effects.                                           |\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "74pskEAJJ-jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to: ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores.\n",
        "#  Random Forest Feature Importance on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1️⃣ Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2️⃣ Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 3️⃣ Get feature importance scores\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# 4️⃣ Sort by importance and get top 5 features\n",
        "top_5_features = importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "# 5️⃣ Print results\n",
        "print(\"Top 5 Most Important Features:\\n\")\n",
        "print(top_5_features.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO-E31B9LBf4",
        "outputId": "6ee1ac4c-bbe1-4c42-ea12-6b8a57c9f1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "\n",
            "             Feature  Importance\n",
            "          worst area    0.139357\n",
            "worst concave points    0.132225\n",
            " mean concave points    0.107046\n",
            "        worst radius    0.082848\n",
            "     worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "#  Bagging Classifier vs Single Decision Tree on Iris Dataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1️⃣ Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2️⃣ Train-Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3️⃣ Train a single Decision Tree\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_pred = tree_model.predict(X_test)\n",
        "tree_acc = accuracy_score(y_test, tree_pred)\n",
        "\n",
        "# 4️⃣ Train a Bagging Classifier with Decision Trees (new syntax)\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  #  updated parameter\n",
        "    n_estimators=50,                      # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# 5️⃣ Print the results\n",
        "print(f\"Single Decision Tree Accuracy: {tree_acc:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy:  {bagging_acc:.2f}\")\n",
        "\n",
        "# Optional: Compare which performed better\n",
        "if bagging_acc > tree_acc:\n",
        "    print(\"\\n Bagging performed better!\")\n",
        "elif bagging_acc < tree_acc:\n",
        "    print(\"\\n Single Decision Tree performed better!\")\n",
        "else:\n",
        "    print(\"\\n Both performed equally.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7BwPTVtYMVEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5184a90-939c-4549-871e-6a6138004af6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.00\n",
            "Bagging Classifier Accuracy:  1.00\n",
            "\n",
            " Both performed equally.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8: Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy\n",
        "#  Random Forest Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1️⃣ Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2️⃣ Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3️⃣ Define the model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 4️⃣ Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],   # number of trees\n",
        "    'max_depth': [None, 5, 10, 15]    # tree depth\n",
        "}\n",
        "\n",
        "# 5️⃣ Perform Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,              # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1          # use all CPU cores\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6️⃣ Best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Final Accuracy on Test Data: {final_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2PubG73dIE0",
        "outputId": "90650ff1-9edb-41db-dee2-1aedca7c4299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy on Test Data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE)\n",
        "#  Bagging Regressor vs Random Forest Regressor on California Housing Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1️⃣ Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# 2️⃣ Train-Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3️⃣ Train Bagging Regressor\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),   #  updated param name for sklearn ≥1.2\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "# 4️⃣ Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# 5️⃣ Print results\n",
        "print(f\"Bagging Regressor MSE:       {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
        "\n",
        "# Optional: Which performed better?\n",
        "if bagging_mse < rf_mse:\n",
        "    print(\"\\n Bagging Regressor performed better!\")\n",
        "elif bagging_mse > rf_mse:\n",
        "    print(\"\\n Random Forest Regressor performed better!\")\n",
        "else:\n",
        "    print(\"\\n⚖️ Both performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZXCCKL0dgnC",
        "outputId": "6fd814e5-2c18-4648-bdc0-0a2422e9d8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE:       0.2573\n",
            "Random Forest Regressor MSE: 0.2573\n",
            "\n",
            "✅ Random Forest Regressor performed better!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "Ans-\n",
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(\"\"\"\n",
        "#  Loan Default Prediction – Ensemble Learning Approach\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ Choosing Between Bagging and Boosting**\n",
        "- **Bagging (Bootstrap Aggregating)**  \n",
        "  - Best when base models have high variance (e.g., deep trees).\n",
        "  - Reduces variance and overfitting.\n",
        "- **Boosting**  \n",
        "  - Best when base models have high bias.\n",
        "  - Sequentially improves weak learners by focusing on mistakes.\n",
        "\n",
        " **Choice:**  \n",
        "Boosting (e.g., XGBoost, LightGBM) is preferred here because:\n",
        "- Handles imbalanced data well.\n",
        "- Focuses on difficult cases.\n",
        "- Works well with tabular financial data.\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ Handling Overfitting**\n",
        "- Limit `max_depth` of trees.\n",
        "- Use smaller `learning_rate` in boosting.\n",
        "- Apply L1/L2 regularization.\n",
        "- Use **early stopping** when validation loss stops improving.\n",
        "\n",
        "---\n",
        "\n",
        "## **3️⃣ Selecting Base Models**\n",
        "- For Bagging: Deep Decision Trees (CART).\n",
        "- For Boosting: Shallow Decision Trees (depth 3–6).\n",
        "- **Why trees?**\n",
        "  - Handle categorical + numerical features.\n",
        "  - Require little preprocessing.\n",
        "  - Capture non-linear relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## **4️⃣ Evaluating Performance (Cross-Validation)**\n",
        "1. Use **Stratified k-Fold Cross-Validation** to keep class balance.\n",
        "2. Metrics to monitor:\n",
        "   - **AUC-ROC** (ranking ability).\n",
        "   - **Precision-Recall** (important for imbalanced data).\n",
        "   - **F1-score** (balance between precision & recall).\n",
        "3. Steps:\n",
        "   - Split into `k` folds.\n",
        "   - Train on `k-1` folds, validate on remaining fold.\n",
        "   - Average metrics over folds.\n",
        "\n",
        "---\n",
        "\n",
        "## **5️⃣ Why Ensemble Learning Helps in Real Life**\n",
        "- **Higher Accuracy** → More reliable predictions of default.\n",
        "- **Risk Reduction** → Avoid high-risk customers, reduce losses.\n",
        "- **Customer Segmentation** → Detect subtle patterns to classify borrowers better.\n",
        "- **Feature Importance Insights** → Identify top factors influencing default for policy making.\n",
        "\n",
        "---\n",
        "\n",
        " **Summary:**\n",
        "- Prefer **Boosting** for this financial task.\n",
        "- Control overfitting with regularization, depth limits, early stopping.\n",
        "- Use **Decision Trees** as base learners.\n",
        "- Evaluate with **Stratified k-Fold CV** + proper metrics.\n",
        "- Ensemble models → better loan approval decisions & reduced financial risk.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "EJ2B8SAiemn2"
      }
    }
  ]
}